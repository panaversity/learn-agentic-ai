# Agent Lifecycle Hooks - Complete Beginner's Guide

## What are Agent Lifecycle Hooks? ğŸ£

Think of **Agent Lifecycle Hooks** like **event listeners** for a specific agent. Just like you might set up notifications for your phone (ring when someone calls, buzz when you get a text), you can set up "listeners" for your agent to know when important things happen.

### ğŸ  House Analogy
Imagine your agent is like a **smart house** that can do different tasks. Agent lifecycle hooks are like **security cameras and sensors** placed specifically in that house to monitor what's happening:

- ğŸ“¹ Camera at the front door (when someone enters)
- ğŸ”§ Sensor in the workshop (when tools are used)  
- ğŸ§  Monitor in the study (when thinking/LLM calls happen)
- ğŸšª Sensor at the exit (when leaving)

## AgentHooksBase Class

This is your "monitoring system" for a **single agent**. You attach this to `agent.hooks` to watch what happens with that specific agent.

### Available Hooks (Event Listeners)

#### 1. `on_start` - Agent Becomes Active ğŸ•˜
```python
async def on_start(context, agent):
    print(f"ğŸ•˜ Agent {agent.name} is now in charge of handling the task")
```

**ğŸ­ Factory Worker Analogy**: Like a worker clocking in and arriving at their workstation. They're now responsible for whatever work comes their way.

**When this happens:**
- The agent becomes the "active" agent in the system
- It's now responsible for handling the current conversation/task
- The agent might receive a handoff from another agent
- This happens **once** when the agent takes control

**Real Example:**
```
User asks: "What's the weather like?"
ğŸ•˜ on_start: WeatherAgent becomes responsible for answering
```

#### 2. `on_end` - Agent Finishes Work âœ…
```python
async def on_end(context, agent, output):
    print(f"âœ… Agent {agent.name} completed work with result: {output}")
```

**ğŸ­ Factory Worker Analogy**: Like a worker finishing their shift and clocking out. They've completed everything they needed to do.

**When this happens:**
- The agent has produced a final answer/result
- The agent's work is completely done
- The agent is about to become inactive
- This is the **last thing** that happens for this agent

**Real Example:**
```
âœ… on_end: WeatherAgent completed with "It's 72Â°F and sunny!"
```

#### 3. `on_llm_start` - Agent Asks the AI Brain ğŸ“
```python
async def on_llm_start(context, agent, system_prompt, input_items):
    print(f"ğŸ“ Agent {agent.name} is asking the AI for help with: {input_items}")
```

**ğŸ­ Factory Worker Analogy**: Like a worker picking up the phone to call an expert consultant. They need advice on how to handle something.

**When this happens:**
- The agent needs to "think" or generate text
- The agent calls the Large Language Model (LLM) for reasoning
- This can happen **multiple times** during one agent's turn
- The agent is asking: "How should I respond?" or "What should I do?"

**Real Example:**
```
ğŸ“ on_llm_start: WeatherAgent asking "How should I get weather for user's location?"
```

#### 4. `on_llm_end` - Agent Gets AI Response ğŸ§ âœ¨
```python
async def on_llm_end(context, agent, response):
    print(f"ğŸ§ âœ¨ Agent {agent.name} got AI response: {response}")
```

**ğŸ­ Factory Worker Analogy**: Like the expert consultant hanging up after giving advice. The worker now knows what to do next.

**When this happens:**
- The LLM has finished "thinking" and provided a response
- The agent now has guidance on what to do next
- This always follows an `on_llm_start`
- The agent might use this advice to take action or call tools

**Real Example:**
```
ğŸ§ âœ¨ on_llm_end: AI responds "Use the weather_lookup tool for their location"
```

#### 5. `on_tool_start` - Agent Uses a Tool ğŸ”¨
```python
async def on_tool_start(context, agent, tool):
    print(f"ğŸ”¨ Agent {agent.name} is using tool: {tool.name}")
```

**ğŸ­ Factory Worker Analogy**: Like a worker reaching for a specific tool from their toolbox to accomplish a task.

**When this happens:**
- The agent needs to perform a specific action
- The agent calls an external function/API/database
- This can happen **multiple times** during one agent's turn
- Common tools: database_lookup, send_email, web_search, calculator

**Real Example:**
```
ğŸ”¨ on_tool_start: WeatherAgent using "weather_lookup" tool
```

#### 6. `on_tool_end` - Agent Finishes Using Tool âœ…ğŸ”¨
```python
async def on_tool_end(context, agent, tool, result):
    print(f"âœ…ğŸ”¨ Agent {agent.name} finished using {tool.name}. Result: {result}")
```

**ğŸ­ Factory Worker Analogy**: Like a worker putting the tool back and seeing the result of their work.

**When this happens:**
- The tool has completed its work and returned a result
- The agent now has new information to work with
- This always follows an `on_tool_start`
- The agent might use this result to make decisions or call the LLM again

**Real Example:**
```
âœ…ğŸ”¨ on_tool_end: weather_lookup returned "72Â°F, Sunny, New York"
```

#### 7. `on_handoff` - Agent Receives Control ğŸƒâ€â™‚ï¸â¡ï¸ğŸƒâ€â™€ï¸
```python
async def on_handoff(context, agent, source):
    print(f"ğŸƒâ€â™‚ï¸â¡ï¸ğŸƒâ€â™€ï¸ Agent {agent.name} received handoff from {source.name}")
```

**ğŸ­ Factory Worker Analogy**: Like one worker finishing their part of an assembly line and passing the work to the next specialized worker.

**When this happens:**
- Another agent has decided this agent should take over
- Work is being transferred from one agent to another
- The receiving agent becomes the new active agent
- This often happens when specialized expertise is needed

**Real Example:**
```
ğŸƒâ€â™‚ï¸â¡ï¸ğŸƒâ€â™€ï¸ TechnicalSupport received handoff from CustomerService
(Customer issue was too complex for general support)
```

## Complete Lifecycle Flow Example ğŸ”„

Here's what a typical conversation looks like with ALL hooks:

```
User: "What's the weather in New York?"

ğŸ•˜ on_start
   â””â”€ WeatherAgent: "I'm now responsible for this weather question"

ğŸ“ on_llm_start  
   â””â”€ WeatherAgent: "AI, how should I handle a weather request?"

ğŸ§ âœ¨ on_llm_end
   â””â”€ AI: "Use the weather_lookup tool for New York"

ğŸ”¨ on_tool_start
   â””â”€ WeatherAgent: "Using weather_lookup tool"

âœ…ğŸ”¨ on_tool_end  
   â””â”€ Tool result: "72Â°F, Sunny, New York"

ğŸ“ on_llm_start (again!)
   â””â”€ WeatherAgent: "AI, how should I format this weather data?"

ğŸ§ âœ¨ on_llm_end
   â””â”€ AI: "Say: 'It's 72Â°F and sunny in New York!'"

âœ… on_end
   â””â”€ WeatherAgent: "Task complete!"
```

## Understanding the Difference: `on_start` vs `on_llm_start` ğŸ¤”

This is the **most confusing part** for beginners! Let's clarify:

**ğŸ•˜ `on_start` - Agent Becomes Responsible**
- Happens **once** when agent takes control
- The agent is now "in charge" of the conversation
- Like clocking in at work

**ğŸ“ `on_llm_start` - Agent Needs to Think**
- Can happen **multiple times** during agent's turn
- The agent asks the AI brain for help with reasoning
- Like calling an expert consultant for advice

**Timeline Example:**
```
ğŸ•˜ on_start: "I'm now handling this customer question"
ğŸ“ on_llm_start: "AI, how should I respond to this?"
âœ… on_llm_end: AI gives advice
ğŸ”§ on_tool_start: Use a tool based on AI advice  
ğŸ“ on_llm_start: "AI, how should I format the results?"
âœ… on_llm_end: AI gives formatting advice
ğŸ on_end: "Task complete!"
```

**Key Difference:** An agent **starts once** but might **ask the AI multiple times**.

## Simple Example

```python
from openai_agents import Agent, AgentHooksBase

# Create a custom hook class for our agent
class MyAgentHooks(AgentHooksBase):
    async def on_start(self, context, agent):
        print(f"ğŸ•˜ {agent.name} is starting up!")
    
    async def on_llm_start(self, context, agent, system_prompt, input_items):
        print(f"ğŸ“ {agent.name} is asking AI for help")
    
    async def on_llm_end(self, context, agent, response):
        print(f"ğŸ§ âœ¨ {agent.name} got AI response")
    
    async def on_tool_start(self, context, agent, tool):
        print(f"ğŸ”¨ {agent.name} is using {tool.name}")
    
    async def on_tool_end(self, context, agent, tool, result):
        print(f"âœ… {agent.name} finished using {tool.name}")
    
    async def on_end(self, context, agent, output):
        print(f"ğŸ‰ {agent.name} completed the task!")

# Create an agent
my_agent = Agent(
    name="Helper Bot",
    # ... other agent configuration
)

# Attach our hooks to this specific agent
my_agent.hooks = MyAgentHooks()
```

## Advanced Example with Detailed Tracking

```python
from openai_agents import Agent, AgentHooksBase
import time
from datetime import datetime

class DetailedAgentHooks(AgentHooksBase):
    def __init__(self):
        self.start_time = None
        self.llm_calls = 0
        self.tool_calls = 0
    
    async def on_start(self, context, agent):
        self.start_time = time.time()
        self.llm_calls = 0
        self.tool_calls = 0
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"ğŸ•˜ [{timestamp}] {agent.name} became active")
    
    async def on_llm_start(self, context, agent, system_prompt, input_items):
        self.llm_calls += 1
        print(f"ğŸ“ LLM Call #{self.llm_calls}: {agent.name} asking AI for guidance")
        print(f"   Input: {len(input_items)} items to think about")
    
    async def on_llm_end(self, context, agent, response):
        print(f"ğŸ§ âœ¨ LLM Call #{self.llm_calls} completed")
        print(f"   AI response length: {len(str(response))} characters")
    
    async def on_tool_start(self, context, agent, tool):
        self.tool_calls += 1
        print(f"ğŸ”¨ Tool #{self.tool_calls}: {agent.name} using {tool.name}")
    
    async def on_tool_end(self, context, agent, tool, result):
        print(f"âœ…ğŸ”¨ Tool #{self.tool_calls} completed")
        print(f"   Result preview: {str(result)[:50]}...")
    
    async def on_handoff(self, context, agent, source):
        print(f"ğŸƒâ€â™‚ï¸â¡ï¸ğŸƒâ€â™€ï¸ {agent.name} received work from {source.name}")
        print(f"   Work is being transferred due to specialization")
    
    async def on_end(self, context, agent, output):
        duration = time.time() - self.start_time if self.start_time else 0
        print(f"âœ… {agent.name} FINISHED in {duration:.2f} seconds")
        print(f"ğŸ“Š Total: {self.llm_calls} AI calls, {self.tool_calls} tool uses")
        print(f"ğŸ¯ Final result: {str(output)[:100]}...")

# Use it with your agent
customer_service = Agent(name="CustomerService")
customer_service.hooks = DetailedAgentHooks()
```

## Hook Execution Order ğŸ“‹

Understanding the order hooks fire helps with debugging:

```
1. ğŸ•˜ on_start (agent becomes active)
2. ğŸ“ on_llm_start (may happen multiple times)
3. ğŸ§ âœ¨ on_llm_end (matches each llm_start)
4. ğŸ”¨ on_tool_start (may happen multiple times)  
5. âœ…ğŸ”¨ on_tool_end (matches each tool_start)
6. ğŸƒâ€â™‚ï¸â¡ï¸ğŸƒâ€â™€ï¸ on_handoff (if agent hands off work)
7. âœ… on_end (agent finishes - only if no handoff)
```

**Note:** Steps 2-5 can repeat multiple times and in different orders!

## Real-World Use Cases ğŸŒ

### 1. Debugging & Development ğŸ›
```python
class DebugHooks(AgentHooksBase):
    async def on_start(self, context, agent):
        print(f"ğŸ› {agent.name} is now active")
    
    async def on_llm_start(self, context, agent, system_prompt, input_items):
        print(f"ğŸ› {agent.name} LLM input: {input_items}")
    
    async def on_tool_start(self, context, agent, tool):
        print(f"ğŸ› {agent.name} using tool: {tool.name}")
```

### 2. User Experience Updates ğŸ‘¥
```python
class UserFriendlyHooks(AgentHooksBase):
    async def on_start(self, context, agent):
        print(f"ğŸ‘‹ {agent.name} is helping you...")
    
    async def on_tool_start(self, context, agent, tool):
        if tool.name == "database_lookup":
            print("ğŸ” Looking up your information...")
        elif tool.name == "send_email":
            print("ğŸ“§ Sending confirmation email...")
    
    async def on_end(self, context, agent, output):
        print("âœ… All done!")
```

## Key Points for Beginners ğŸ“š

1. **Agent-Specific**: These hooks only watch **one specific agent**
2. **Individual Monitoring**: Each agent can have its own set of hooks
3. **Attachment**: You attach hooks using `agent.hooks = YourHooksClass()`
4. **Async**: All hook methods must be `async` functions
5. **Multiple Calls**: `on_llm_start` and `on_tool_start` can happen multiple times
6. **Paired Events**: Every `start` hook has a corresponding `end` hook

## Common Mistakes âŒ

### âŒ Don't Do This:
```python
# Forgetting async
def on_start(self, context, agent):  # Missing 'async'!
    print("Started")

# Trying to return values from hooks
async def on_tool_end(self, context, agent, tool, result):
    return "modified result"  # Hooks don't return values!
```

### âœ… Do This Instead:
```python
async def on_start(self, context, agent):  # Proper async
    print("Started")

async def on_tool_end(self, context, agent, tool, result):
    # Just observe and log, don't try to modify
    print(f"Tool {tool.name} returned: {result}")
```

Think of Agent hooks as putting a **personal assistant** next to each of your agents to take detailed notes on everything they do! ğŸ“ğŸ‘¨â€ğŸ’¼