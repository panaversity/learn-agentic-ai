{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdKwzEluDBN7"
      },
      "source": [
        "### Install openai-agents SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QdkOviEB2ay",
        "outputId": "d77b08ca-7a76-4d04-e458-ca4bc4f6f4a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m506.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.4/161.4 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.5/132.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.5/158.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -Uq openai-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yD91lz4DIAx"
      },
      "source": [
        "### Make your Jupyter Notebook capable of running asynchronous functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7A5YLi3HCfBV"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJoDmfdcUm81"
      },
      "source": [
        "### Get your LLM API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLjHL03bT8_D"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "gemini_api_key = userdata.get(\"GEMINI_API_KEY\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🔑 Setup OpenAI API Key & Enable Tracing in Agents SDK\n",
        "\n",
        "To enable **traces/logs** in your Agents SDK setup, you’ll need an **OpenAI API Key**.  \n",
        "Follow these steps:\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Create OpenAI Account\n",
        "- Go to [OpenAI Platform](https://platform.openai.com/).\n",
        "- Sign up / Log in.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Generate API Key\n",
        "- After login, navigate to:  \n",
        "  👉 [API Keys](https://platform.openai.com/account/api-keys)  \n",
        "- Click **“Create new secret key”**.\n",
        "- Copy your new key safely.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Add API Key in `.env` file\n",
        "Inside your project, create a `.env` file and add:\n",
        "\n",
        "```env\n",
        "OPENAI_API_KEY=your_api_key_here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2dv85_EUwVT"
      },
      "source": [
        "## How to configure LLM Providers at different levels (Global, Run and Agent)?\n",
        "\n",
        "Agents SDK is setup to use OpenAI as default providers. When using other providers you can setup at different levels:\n",
        "1. Agent Level\n",
        "2. RUN LEVEL\n",
        "3. Global Level\n",
        "\n",
        "We will always your Agent Level Configuration so each agent can use the LLM best fit for it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6mLXIwYTxwK"
      },
      "source": [
        "### 1. AGENT LEVEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEkPY49XTwbP",
        "outputId": "74de1603-a88d-411d-fd89-6813186f5acd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "یہ بہت اچھی بات ہے۔ ایجنٹک AI ایک دلچسپ اور ابھرتا ہوا شعبہ ہے۔ امید ہے آپ پاناورسٹی کمیونٹی کے ساتھ اچھا سیکھ رہے ہوں گے۔\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from openai import AsyncOpenAI\n",
        "from agents import Agent, OpenAIChatCompletionsModel, Runner\n",
        "\n",
        "\n",
        "#Reference: https://ai.google.dev/gemini-api/docs/openai\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "async def main():\n",
        "    # This agent will use the custom LLM provider\n",
        "    agent = Agent(\n",
        "        name=\"Assistant\",\n",
        "        instructions=\"You only respond in urdu.\",\n",
        "        model=OpenAIChatCompletionsModel(model=\"gemini-2.5-flash\", openai_client=client),\n",
        "    )\n",
        "\n",
        "    result = await Runner.run(\n",
        "        agent,\n",
        "        \"I am learning Agentic AI with Panaversity Community\",\n",
        "    )\n",
        "    print(result.final_output)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb5cWUWAURHe"
      },
      "source": [
        "### 2. RUN LEVEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vUgBE2jUPi7",
        "outputId": "ea506ee7-31ad-4e9e-df89-967defce5e95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! As a large language model, I don't experience feelings like humans do. However, I am functioning optimally and ready to assist you. How can I help you today?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel\n",
        "from agents.run import RunConfig\n",
        "\n",
        "#Reference: https://ai.google.dev/gemini-api/docs/openai\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=external_client\n",
        ")\n",
        "\n",
        "config = RunConfig(\n",
        "    model=model,\n",
        "    model_provider=external_client,\n",
        "    tracing_disabled=True\n",
        ")\n",
        "\n",
        "agent: Agent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\")\n",
        "\n",
        "result = Runner.run_sync(agent, \"Hello, how are you.\", run_config=config)\n",
        "\n",
        "print(result.final_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb9QgpqKUaIi"
      },
      "source": [
        "### GLOBAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHLr1P7vUWhn",
        "outputId": "4fdc6140-867a-4178-b0ac-69c5fe164b2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hi there! How can I help you today?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from agents import Agent, Runner, AsyncOpenAI, set_default_openai_client, set_tracing_disabled, set_default_openai_api,set_tracing_export_api_key\n",
        "\n",
        "# 🔹 Enable/Disable tracing logs (set to False if you want to see traces on OpenAI platform)\n",
        "# set_tracing_disabled(True)\n",
        "\n",
        "set_default_openai_api(\"chat_completions\")\n",
        "\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "set_default_openai_client(external_client)\n",
        "\n",
        "# 🔹 Export traces/logs to OpenAI using your OpenAI API key\n",
        "# NOTE: This only needs to be set at the global level\n",
        "set_tracing_export_api_key(openai_api_key)\n",
        "\n",
        "agent: Agent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\", model=\"gemini-2.0-flash\")\n",
        "\n",
        "result = Runner.run_sync(agent, \"Hello\")\n",
        "\n",
        "print(result.final_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sai9zXdAu4-t"
      },
      "source": [
        "### Set debug mode on (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bdWaK-w8u3dD"
      },
      "outputs": [],
      "source": [
        "from agents import enable_verbose_stdout_logging\n",
        "\n",
        "enable_verbose_stdout_logging()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t47o3UhOUf3v"
      },
      "source": [
        "> This is for Debugging and looking what happens inside of Agent SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi8QzcUaUewY",
        "outputId": "7c5e31ea-d300-4538-b4d9-1bba66d8be85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tracing is disabled. Not creating trace Agent workflow\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:openai.agents:Tracing is disabled. Not creating trace Agent workflow\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting current trace: no-op\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:openai.agents:Setting current trace: no-op\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x788e75bc2c30>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:openai.agents:Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x788e75bc2c30>\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running agent Assistant (turn 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:openai.agents:Running agent Assistant (turn 1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x788e75c4a090>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:openai.agents:Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x788e75c4a090>\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    \"content\": \"You are a helpful assistant\",\n",
            "    \"role\": \"system\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"user\",\n",
            "    \"content\": \"Hello\"\n",
            "  }\n",
            "]\n",
            "Tools:\n",
            "[]\n",
            "Stream: False\n",
            "Tool choice: NOT_GIVEN\n",
            "Response format: NOT_GIVEN\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:openai.agents:[\n",
            "  {\n",
            "    \"content\": \"You are a helpful assistant\",\n",
            "    \"role\": \"system\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"user\",\n",
            "    \"content\": \"Hello\"\n",
            "  }\n",
            "]\n",
            "Tools:\n",
            "[]\n",
            "Stream: False\n",
            "Tool choice: NOT_GIVEN\n",
            "Response format: NOT_GIVEN\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM resp:\n",
            "{\n",
            "  \"content\": \"Hello! How can I help you today?\\n\",\n",
            "  \"refusal\": null,\n",
            "  \"role\": \"assistant\",\n",
            "  \"annotations\": null,\n",
            "  \"audio\": null,\n",
            "  \"function_call\": null,\n",
            "  \"tool_calls\": null\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:openai.agents:LLM resp:\n",
            "{\n",
            "  \"content\": \"Hello! How can I help you today?\\n\",\n",
            "  \"refusal\": null,\n",
            "  \"role\": \"assistant\",\n",
            "  \"annotations\": null,\n",
            "  \"audio\": null,\n",
            "  \"function_call\": null,\n",
            "  \"tool_calls\": null\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resetting current trace\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:openai.agents:Resetting current trace\n"
          ]
        }
      ],
      "source": [
        "result = Runner.run_sync(agent, \"Hello\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
